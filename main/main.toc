\babel@toc {dutch}{}\relax 
\contentsline {chapter}{Voorwoord}{I}{chapter*.1}%
\contentsline {chapter}{Inhoudsopgave}{II}{chapter*.2}%
\contentsline {chapter}{\numberline {1}Inleiding}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Doel van het onderzoek}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Onderzoeksvragen}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Hypothese}{3}{section.1.3}%
\contentsline {section}{\numberline {1.4}Relevantie van het Onderzoek}{3}{section.1.4}%
\contentsline {chapter}{\numberline {2}Theoretisch Kader}{4}{chapter.2}%
\contentsline {section}{\numberline {2.1}Fundamentele Elementen van MDP's}{4}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Toestandsruimte}{4}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Actieruimte}{4}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Beloningsfunctie}{4}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Markov-eigenschap en Overgangsdynamiek}{5}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}De Markov-eigenschap}{5}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Overgangswaarschijnlijkheidsfunctie}{5}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Beleid en Verwachte Waarden}{5}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Beleid}{5}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Verwachte Waarden}{6}{subsection.2.3.2}%
\contentsline {section}{\numberline {2.4}Leerparameters in Reinforcement Learning}{6}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Leerpercentage}{6}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Kortingsfactor}{6}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Exploratieparameter}{7}{subsection.2.4.3}%
\contentsline {section}{\numberline {2.5}Waarde-functies}{7}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}Toestandswaarde-functie}{7}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}Q-functie}{7}{subsection.2.5.2}%
\contentsline {chapter}{\numberline {3}Kenmerken van specifieke Algoritmes}{8}{chapter.3}%
\contentsline {section}{\numberline {3.1}Q-Learning}{8}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Proces}{8}{subsection.3.1.1}%
\contentsline {subsection}{\numberline {3.1.2}Beperkingen}{10}{subsection.3.1.2}%
\contentsline {section}{\numberline {3.2}Deep Q-Network}{10}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Neuraal Netwerk}{10}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Proces}{12}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Verbeteringen op Klassiek Q-learning}{12}{subsection.3.2.3}%
\contentsline {subsection}{\numberline {3.2.4}Voordelen en Beperkingen}{12}{subsection.3.2.4}%
\contentsline {subsection}{\numberline {3.2.5}Toepassingen}{14}{subsection.3.2.5}%
\contentsline {section}{\numberline {3.3}Deep Policy Gradient}{14}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Actor-Critic model}{15}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Proces}{15}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Toepassingen}{16}{subsection.3.3.3}%
\contentsline {chapter}{\numberline {4}Kenmerken van specifieke Computerspellen}{17}{chapter.4}%
\contentsline {section}{\numberline {4.1}Indeling en Strategische Diepgang van Spellen}{17}{section.4.1}%
\contentsline {section}{\numberline {4.2}Indeling van Spellen}{18}{section.4.2}%
\contentsline {section}{\numberline {4.3}Strategische Diepgang}{18}{section.4.3}%
\contentsline {section}{\numberline {4.4}Beslissingsdynamiek en Tijdgevoeligheid}{19}{section.4.4}%
\contentsline {section}{\numberline {4.5}Complexiteit}{19}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Regels en Beperkingen}{19}{subsection.4.5.1}%
\contentsline {subsubsection}{Spellen met veel regels en vaste patronen}{19}{subsection.4.5.1}%
\contentsline {subsubsection}{Spellen met weinig regels en veel vrijheid}{20}{subsection.4.5.1}%
\contentsline {section}{\numberline {4.6}Dynamiek en Tijdgevoeligheid}{20}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Turn-based spellen}{20}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Realtime spellen}{20}{subsection.4.6.2}%
\contentsline {section}{\numberline {4.7}Beloningsstructuur}{20}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Directe beloningen}{20}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Cumulatieve beloningen}{21}{subsection.4.7.2}%
\contentsline {chapter}{\numberline {5}De invloed van spelkenmerken op Reinforcement Learning-Algoritmes}{22}{chapter.5}%
\contentsline {section}{\numberline {5.1}Strategische Diepgang}{22}{section.5.1}%
\contentsline {section}{\numberline {5.2}Regels en Beperkingen}{23}{section.5.2}%
\contentsline {section}{\numberline {5.3}Dynamiek en Tijdgevoeligheid}{23}{section.5.3}%
\contentsline {section}{\numberline {5.4}Beloningsstructuur}{24}{section.5.4}%
\contentsline {section}{\numberline {5.5}Complexiteit van de Toestandsruimte}{24}{section.5.5}%
\contentsline {section}{\numberline {5.6}Onvoorspelbaarheid}{25}{section.5.6}%
\contentsline {chapter}{\numberline {6}Onderzoeksmethoden}{26}{chapter.6}%
\contentsline {section}{\numberline {6.1}Technische Uitvoering}{26}{section.6.1}%
\contentsline {section}{\numberline {6.2}Verzamelen van Gegevens}{26}{section.6.2}%
\contentsline {section}{\numberline {6.3}Optimalisatie van Instellingen}{26}{section.6.3}%
\contentsline {chapter}{\numberline {7}Resultaten}{28}{chapter.7}%
\contentsline {section}{\numberline {7.1}Experimentele Opzet}{28}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}Hyperparameters}{28}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2}Prestatievergelijking: Gemiddelde Score per Episode}{28}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Samenvatting van Prestaties}{28}{subsection.7.2.1}%
\contentsline {section}{\numberline {7.3}Leerdynamiek: Loss en Entropy}{29}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Loss-curves}{29}{subsection.7.3.1}%
\contentsline {subsection}{\numberline {7.3.2}Entropy (PPO)}{30}{subsection.7.3.2}%
\contentsline {section}{\numberline {7.4}Gedragsanalyse: Beleid in Actie}{30}{section.7.4}%
\contentsline {subsection}{\numberline {7.4.1}Heatmap van Q-waarden (Q-Learning)}{30}{subsection.7.4.1}%
\contentsline {subsection}{\numberline {7.4.2}Trajectory Plot (DQN vs. PPO)}{31}{subsection.7.4.2}%
\contentsline {section}{\numberline {7.5}Falen en Uitdagingen}{31}{section.7.5}%
\contentsline {section}{\numberline {7.6}Benchmark tegen Menselijke Prestaties}{32}{section.7.6}%
\contentsline {section}{\numberline {7.7}Conclusies uit Resultaten}{32}{section.7.7}%
\contentsline {chapter}{\numberline {A}Notatie}{34}{appendix.A}%
\contentsline {chapter}{\numberline {B}Induveele Resultaten Algoritmes}{36}{appendix.B}%
\contentsline {section}{\numberline {B.1}Experimentele Opzet}{36}{section.B.1}%
\contentsline {section}{\numberline {B.2}Prestaties van Q-Learning}{36}{section.B.2}%
\contentsline {section}{\numberline {B.3}Prestaties van Deep Q-Network (DQN)}{37}{section.B.3}%
\contentsline {section}{\numberline {B.4}Prestaties van Proximal Policy Optimization (PPO)}{37}{section.B.4}%
\contentsline {section}{\numberline {B.5}Vergelijkende Analyse}{38}{section.B.5}%
\contentsline {section}{\numberline {B.6}Visuele Weergave van Agentgedrag}{38}{section.B.6}%
\contentsline {chapter}{\numberline {C}Logboek}{40}{appendix.C}%
\contentsline {section}{\numberline {C.1}Groepsactiviteiten}{40}{section.C.1}%
\contentsline {section}{\numberline {C.2}Matthijs}{41}{section.C.2}%
\contentsline {section}{\numberline {C.3}Thom}{42}{section.C.3}%
\contentsline {section}{\numberline {C.4}Pepijn}{43}{section.C.4}%
\contentsline {chapter}{Bibliografie}{45}{appendix*.23}%
